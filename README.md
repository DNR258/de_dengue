# Dengue cases in Argentine and the world
Data pipeline for Dengue data visualization

## Technologies
In this project I used:

- Terraform as Infrastructure-as-Code (IaC) for cloud setup,
- Mage AI for orchestration,
- Google Cloud Storage (GCS) as data lake for data storage,
- Google BigQuery as Data Warehouse,
- dbt for the transformation of data the data,
- Google Looker studio for visualizations.

The data pipeline pulls Dengue data from two different sources, loads it into GCS and then into BigQuery using Mage. After that, the data is transformed inside the data warehouse with dbt, and visualized in Looker Studio:

![Mi primer tablero (1)](https://github.com/DNR258/de_dengue/assets/97068501/e1af016c-1c3a-449a-877c-8624496a79b1)

## Problem Description
I choose two different data sources with the goal of build a detailed register of reported Dengue cases around the world with special emphasis on those registered on Argentine. One of the data sources contains Dengue data from all over the world, but the Argentinian data is updated just until 2022. So, I decided to complete this data with public yearly data generated by the Argentinian Health authorities. Since the data corresponding to the current year is not available yet, the pipeline is going to be refreshed anually. Although it is posible to get the 2024 data, the available format of it difficult its extraction and processing, so it will be a future challenge. 

Data sources differs in how the data is presented and show some heterogeneity in the data, adding some difficulty to its integration. Following are listed some of the issues faced during the project:
- The same data source present data in a yearly, monthly, or even in a weekly basis,
- when data is presented in a weekly basis, the week could start on sunday or monday depending of the country,
- one dataset present data with start and end dates, while the other present data in epidemiological weeks enumerated from 1 to 52 (or 53),
- sometimes the amount of cases for a given period is inconsistent depending of what unit of measure is used (week, month or year). In those cases I decided to sum the amount of cases, and compared between them, after that the unit of measure with the bigger amount of cases was taken as the correct one. The results where checked with data online and the result was satisfactory.   

## Repository Structure
- dbt: all directories, .yml, .scl files, etc. for the transformations in dbt
- mage: .py and .sql files for the mage flows
- terraform: terraform files


## Do it yourself
To reproduce this pipeline just follow the instructions.

## Visualizations
![report](https://github.com/DNR258/de_dengue/assets/97068501/76a50cb4-662e-48fe-9078-6eccf12fb78d)
